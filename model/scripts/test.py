import torch
import numpy as np
from sklearn.metrics import classification_report, confusion_matrix
import os
import json

import sys

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from config.config import config
from models.armor_cnn import create_model
from utils.data_loader import get_test_loader
from utils.visualization import plot_confusion_matrix, visualize_predictions


def test_model(model_path):
    """æµ‹è¯•æ¨¡å‹"""
    # åŠ è½½æ¨¡å‹
    model = create_model(num_classes=config.NUM_CLASSES)

    if os.path.exists(model_path):
        checkpoint = torch.load(model_path, map_location=config.DEVICE)
        if isinstance(checkpoint, dict) and "model_state_dict" in checkpoint:
            model.load_state_dict(checkpoint["model_state_dict"])
            if "accuracy" in checkpoint:
                print(f"æ¨¡å‹è®­ç»ƒå‡†ç¡®ç‡: {checkpoint['accuracy']:.2f}%")
        else:
            model.load_state_dict(checkpoint)
    else:
        raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {model_path}")

    model = model.to(config.DEVICE)
    model.eval()

    print(f"åŠ è½½æ¨¡å‹: {model_path}")
    print(f"æµ‹è¯•è®¾å¤‡: {config.DEVICE}")

    # è·å–æµ‹è¯•æ•°æ®åŠ è½½å™¨
    test_loader = get_test_loader()

    # æµ‹è¯•
    all_predictions = []
    all_labels = []
    all_probabilities = []

    with torch.no_grad():
        for images, labels in test_loader:
            images, labels = images.to(config.DEVICE), labels.to(config.DEVICE)
            outputs = model(images)
            probabilities = torch.softmax(outputs, dim=1)
            _, predictions = torch.max(outputs, 1)

            all_predictions.extend(predictions.cpu().numpy())
            all_labels.extend(labels.cpu().numpy())
            all_probabilities.extend(probabilities.cpu().numpy())

    # è®¡ç®—å‡†ç¡®ç‡
    accuracy = np.mean(np.array(all_predictions) == np.array(all_labels)) * 100

    # æ‰“å°ç»“æœ
    print(f"\nğŸ“Š æµ‹è¯•ç»“æœ:")
    print(f"æ•´ä½“å‡†ç¡®ç‡: {accuracy:.2f}%")

    # æ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡
    class_names = [str(i) for i in range(1, config.NUM_CLASSES + 1)]
    print(f"\næ¯ä¸ªç±»åˆ«çš„å‡†ç¡®ç‡:")
    class_accuracy = {}
    for i in range(config.NUM_CLASSES):
        class_mask = np.array(all_labels) == i
        if np.sum(class_mask) > 0:
            class_acc = np.mean(np.array(all_predictions)[class_mask] == i) * 100
            class_accuracy[f"class_{i + 1}"] = class_acc
            print(f"  æ•°å­— {i + 1}: {class_acc:.2f}% ({np.sum(class_mask)} å¼ å›¾ç‰‡)")

    # åˆ†ç±»æŠ¥å‘Š
    print(f"\nè¯¦ç»†åˆ†ç±»æŠ¥å‘Š:")
    print(
        classification_report(
            all_labels, all_predictions, target_names=class_names, digits=4
        )
    )

    # ç»˜åˆ¶æ··æ·†çŸ©é˜µ
    cm_path = os.path.join(config.LOG_DIR, "confusion_matrix.png")
    plot_confusion_matrix(all_labels, all_predictions, class_names, save_path=cm_path)

    # å¯è§†åŒ–ä¸€äº›é¢„æµ‹ç»“æœ
    viz_path = os.path.join(config.LOG_DIR, "predictions_visualization.png")
    visualize_predictions(model, test_loader, class_names, save_path=viz_path)

    # ä¿å­˜æµ‹è¯•ç»“æœ
    results = {
        "overall_accuracy": accuracy,
        "class_accuracy": class_accuracy,
        "total_samples": len(all_labels),
        "predictions": all_predictions,
        "labels": all_labels,
    }

    results_path = os.path.join(config.LOG_DIR, "test_results.json")
    with open(results_path, "w") as f:
        json.dump(
            {k: v for k, v in results.items() if k not in ["predictions", "labels"]},
            f,
            indent=2,
        )

    print(f"\næµ‹è¯•ç»“æœå·²ä¿å­˜åˆ°: {results_path}")

    return accuracy, all_predictions, all_labels


def main():
    # æµ‹è¯•æœ€ä½³æ¨¡å‹
    best_model_path = os.path.join(config.MODEL_SAVE_DIR, "best_model.pth")

    if os.path.exists(best_model_path):
        test_model(best_model_path)
    else:
        print(f"æœªæ‰¾åˆ°æ¨¡å‹æ–‡ä»¶: {best_model_path}")
        print("è¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬ train.py")


if __name__ == "__main__":
    main()
